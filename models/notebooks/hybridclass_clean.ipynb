{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10597785,"sourceType":"datasetVersion","datasetId":6559539},{"sourceId":243753,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":208219,"modelId":229916},{"sourceId":243769,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":208234,"modelId":229930},{"sourceId":262709,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":224664,"modelId":246412},{"sourceId":270758,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":208234,"modelId":229930}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import random_split\nfrom torch.autograd import Variable\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import precision_recall_fscore_support, confusion_matrix\nimport numpy as np\nimport torchvision\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-28T06:40:33.867455Z","iopub.execute_input":"2025-02-28T06:40:33.867738Z","iopub.status.idle":"2025-02-28T06:40:52.390570Z","shell.execute_reply.started":"2025-02-28T06:40:33.867709Z","shell.execute_reply":"2025-02-28T06:40:52.389689Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from transformers import CLIPProcessor, CLIPModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T06:40:52.391508Z","iopub.execute_input":"2025-02-28T06:40:52.391957Z","iopub.status.idle":"2025-02-28T06:40:54.333555Z","shell.execute_reply.started":"2025-02-28T06:40:52.391935Z","shell.execute_reply":"2025-02-28T06:40:54.332646Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T06:40:54.334441Z","iopub.execute_input":"2025-02-28T06:40:54.334908Z","iopub.status.idle":"2025-02-28T06:40:54.396838Z","shell.execute_reply.started":"2025-02-28T06:40:54.334886Z","shell.execute_reply":"2025-02-28T06:40:54.395936Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import kagglehub\npath_to_clipped_freqnet = kagglehub.model_download('aayushpuri01/clipped-freqnet/PyTorch/default/1')\nprint(path_to_clipped_freqnet)\nos.chdir(path_to_clipped_freqnet)\nfrom clipped_freqnet import freqnet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T06:40:54.398681Z","iopub.execute_input":"2025-02-28T06:40:54.398900Z","iopub.status.idle":"2025-02-28T06:40:55.750005Z","shell.execute_reply.started":"2025-02-28T06:40:54.398881Z","shell.execute_reply":"2025-02-28T06:40:55.749253Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/clipped-freqnet/pytorch/default/1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"path_to_dataloader = kagglehub.model_download('aayushpuri01/dataloader-pipeline-for-dds/PyTorch/default/2')\nprint(path_to_dataloader)\nos.chdir(path_to_dataloader)\nfrom dataloading_pipeline_fixleakage import DataPipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T06:40:55.750835Z","iopub.execute_input":"2025-02-28T06:40:55.751382Z","iopub.status.idle":"2025-02-28T06:40:56.348084Z","shell.execute_reply.started":"2025-02-28T06:40:55.751334Z","shell.execute_reply":"2025-02-28T06:40:56.347218Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/dataloader-pipeline-for-dds/pytorch/default/2\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"os.chdir('/kaggle/working')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T06:40:56.348867Z","iopub.execute_input":"2025-02-28T06:40:56.349085Z","iopub.status.idle":"2025-02-28T06:40:56.352713Z","shell.execute_reply.started":"2025-02-28T06:40:56.349064Z","shell.execute_reply":"2025-02-28T06:40:56.351813Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Wandb config","metadata":{}},{"cell_type":"code","source":"import wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=api_key)\n    anony = None\nexcept:\n    anony = \"must\"\n    \n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:13:34.271844Z","iopub.execute_input":"2025-02-28T07:13:34.272128Z","iopub.status.idle":"2025-02-28T07:13:43.579974Z","shell.execute_reply.started":"2025-02-28T07:13:34.272107Z","shell.execute_reply":"2025-02-28T07:13:43.579109Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maayushpuri2486\u001b[0m (\u001b[33maayushpuri2486-pulchowk-campus\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"wandb.init(\n    # set the wandb project where this run will be logged\n    project=\"before shipping notebook test\",\n\n    # track hyperparameters and run metadata\n    config={\n    \"learning_rate\": 0.0001,\n    \"architecture\": \"Freqnet+CLIP\",\n    \"dataset\": \"Custom Augmented Dataset\",\n    \"epochs\": 1,\n    }\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataloader Configuration","metadata":{}},{"cell_type":"markdown","source":"The DataPipeline function takes the following arguments:\n\n1. ***path_to_drive*** and ***data_dir*** are joined to make the complete relative directory path,\n   so just split if you have a complete path\n2. ***num_images*** (default value set to 20,000): create a subset of the dataset, i.e. 20,000 images will\n   be used for training and validation set\n3. ***val_split*** set for the portion of limited dataset to use for validation\n4. ***batch_size***\n5. ***test_size*** (default set to 3000): creates a separate test set of 3000 images. \n\nValues can be changed for experimentations. \n","metadata":{}},{"cell_type":"code","source":"pipeline = DataPipeline(path_to_drive='/kaggle/input',\n                        data_dir='deepfake-dataset/Deepfake_Dataset',\n                        val_split=0.3,\n                        batch_size=32,\n                        num_images=1000,\n                        test_size =100)\ntrain_loader, val_loader, test_loader = pipeline.get_loaders()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:11:44.362968Z","iopub.execute_input":"2025-02-28T07:11:44.363322Z","iopub.status.idle":"2025-02-28T07:12:08.846742Z","shell.execute_reply.started":"2025-02-28T07:11:44.363291Z","shell.execute_reply":"2025-02-28T07:12:08.846045Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"#Visualising some images from the dataloaders, \n#try all the data loaders \n#comment the cell when running the session \n\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ndef denormalized(img_tensor, mean, std):\n    img_tensor = img_tensor.clone()  # cloning the tensor to avoid changing it\n    for t, m, s in zip(img_tensor, mean, std):\n        t.mul_(s).add_(m)  # Denormalize each channel\n    return img_tensor\n\ndata_iter = iter(val_loader)\nimages, labels = next(data_iter)\nimages = torch.stack([denormalized(img, mean, std) for img in images])\nimages = images.numpy().transpose((0, 2, 3, 1))\n\nfig, axes = plt.subplots(3, 3, figsize = (8, 6))\nfor i in range(3):\n  for j in range(3):\n    axes[i][j].imshow(images[i*3+j])\n    lbl = labels[i*3+j].item()\n    if lbl == 0:\n      axes[i][j].set_title(\"Fake\")\n    else:\n      axes[i][j].set_title(\"Real\")\n    axes[i][j].axis('off')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pipeline = DataPipeline(path_to_drive='/kaggle/input',\n#                         data_dir='deepfake-dataset/Deepfake_Dataset',\n#                         val_split=0.3,\n#                         batch_size=32,\n#                         num_images=20000)\n# train_loader, val_loader, test_loader = pipeline.get_loaders()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T05:17:17.729228Z","iopub.execute_input":"2025-02-22T05:17:17.729513Z","iopub.status.idle":"2025-02-22T05:17:40.176534Z","shell.execute_reply.started":"2025-02-22T05:17:17.729491Z","shell.execute_reply":"2025-02-22T05:17:40.175527Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Model Architecture Config and Init","metadata":{}},{"cell_type":"code","source":"class HybridDeepfakeDetector(nn.Module):\n    def __init__(self, freq_model, clip_model_name=\"openai/clip-vit-large-patch14\", device=\"cuda\"):\n        super(HybridDeepfakeDetector, self).__init__()\n        self.device = device\n\n        # Load FreqNet\n        self.freqnet = freq_model.to(self.device)\n\n        # Load CLIP Model\n        self.clip_model = CLIPModel.from_pretrained(clip_model_name).to(self.device)\n        self.clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n\n        # Define Fully Connected Classifier\n        self.fc = nn.Linear(512 + 768, 1)  # FreqNet (512) + CLIP (768)\n\n        # ImageNet Denormalization\n        imagenet_mean = [0.485, 0.456, 0.406]\n        imagenet_std = [0.229, 0.224, 0.225]\n        self.denormalize = transforms.Compose([\n            transforms.Normalize(mean=[-m / s for m, s in zip(imagenet_mean, imagenet_std)],\n                                 std=[1 / s for s in imagenet_std]),\n            # transforms.Lambda(lambda x: x.clamp(0, 1))  # Clamp to [0,1]\n            transforms.Lambda(HybridDeepfakeDetector.clamp_image)\n        ])\n\n    @staticmethod\n    def clamp_image(tensor):\n        return tensor.clamp(0, 1)\n    \n    def extract_clip_features(self, images):\n        \"\"\"\n        Extract CLIP embeddings from images.\n        \"\"\"\n        inputs = self.clip_processor(images=images, return_tensors=\"pt\", padding=True)\n        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n        with torch.no_grad():\n            embeddings = self.clip_model.get_image_features(**inputs)\n        return embeddings.to(self.device)\n\n    def forward(self, freq_input, clip_input):\n        \"\"\"\n        freq_input: Image tensors normalized with ImageNet stats for FreqNet\n        clip_input: Same image tensors (but denormalized) for CLIP\n        \"\"\"\n        # Get frequency-based features from FreqNet\n        freq_features = self.freqnet(freq_input)\n\n        # Get semantic embeddings from CLIP\n        denormalized_images = torch.stack([self.denormalize(image) for image in clip_input])\n        clip_features = self.extract_clip_features(denormalized_images)\n\n        # Normalize features and concatenate\n        freq_features = F.normalize(freq_features, dim=-1)\n        clip_features = F.normalize(clip_features, dim=-1)\n        combined_features = torch.cat((freq_features, clip_features), dim=-1)\n\n        # Classifier output\n        logits = self.fc(combined_features)\n        return logits.squeeze()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:14:26.139446Z","iopub.execute_input":"2025-02-28T07:14:26.139826Z","iopub.status.idle":"2025-02-28T07:14:26.151587Z","shell.execute_reply.started":"2025-02-28T07:14:26.139801Z","shell.execute_reply":"2025-02-28T07:14:26.150724Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"#Initialize the models\nfreqnet_model = freqnet(num_classes=2)  # Assuming `freqnet` function initializes FreqNet\nmodel = HybridDeepfakeDetector(freqnet_model, device=device).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:14:26.334104Z","iopub.execute_input":"2025-02-28T07:14:26.334436Z","iopub.status.idle":"2025-02-28T07:14:28.703399Z","shell.execute_reply.started":"2025-02-28T07:14:26.334410Z","shell.execute_reply":"2025-02-28T07:14:28.702740Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Optimizer & Loss\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0001) #set lr = 0.001 optimally? \ncriterion = nn.BCEWithLogitsLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:14:30.456199Z","iopub.execute_input":"2025-02-28T07:14:30.456533Z","iopub.status.idle":"2025-02-28T07:14:30.465216Z","shell.execute_reply.started":"2025-02-28T07:14:30.456506Z","shell.execute_reply":"2025-02-28T07:14:30.464380Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"# Helper Functions","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(epoch, model, optimizer, criterion, train_loader, device):\n    model.train()\n    running_loss = 0.0\n    running_corrects = 0\n\n    loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=True)\n    \n    for batch_idx, (images, labels) in loop:\n        images, labels = images.to(device), labels.to(device).float()\n\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(images, images)  # Passing same images to both FreqNet & CLIP\n        loss = criterion(outputs, labels)\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n\n        # Track loss and accuracy\n        running_loss += loss.item() * images.size(0)\n        predicted = (torch.sigmoid(outputs) > 0.5).float()\n        running_corrects += (predicted == labels).sum().item()\n\n        loop.set_description(f'Epoch [{epoch+1}]')\n        loop.set_postfix(loss=loss.item())\n\n    epoch_loss = running_loss / len(train_loader.dataset)\n    epoch_acc = running_corrects / len(train_loader.dataset)\n    print(f'Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.4f}')\n    return epoch_loss, epoch_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:15:02.712466Z","iopub.execute_input":"2025-02-28T07:15:02.712770Z","iopub.status.idle":"2025-02-28T07:15:02.719786Z","shell.execute_reply.started":"2025-02-28T07:15:02.712748Z","shell.execute_reply":"2025-02-28T07:15:02.719014Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def validate_one_epoch(epoch, model, criterion, val_loader, device):\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        progress_bar = tqdm(val_loader, desc=\"Validating\", leave=False)\n        for images, labels in progress_bar:\n            images, labels = images.to(device), labels.to(device).float()\n            outputs = model(images, images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * images.size(0)\n\n            # Predictions\n            predicted = (torch.sigmoid(outputs) > 0.5).float()\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            progress_bar.set_postfix({'Val Loss': loss.item()})\n\n    val_acc = correct / total\n    print(f'Validation Loss: {val_loss / len(val_loader.dataset):.4f}, Validation Accuracy: {val_acc:.4f}')\n    return val_loss / len(val_loader.dataset), val_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:15:06.333147Z","iopub.execute_input":"2025-02-28T07:15:06.333453Z","iopub.status.idle":"2025-02-28T07:15:06.340116Z","shell.execute_reply.started":"2025-02-28T07:15:06.333429Z","shell.execute_reply":"2025-02-28T07:15:06.339225Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def save_checkpoint(epoch, model, optimizer, checkpoint_dir = '/kaggle/working'):\n    os.makedirs(checkpoint_dir, exist_ok = True)\n\n    path_to_checkpoint = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch+1}.pth\")\n\n    #saving the checkpoint dictionary with the model and optimizer state dict only for resuming training if deemed necesary\n\n    torch.save({\n        \"epoch\": epoch + 1, \n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n    }, path_to_checkpoint)\n\n    #might as well log to wandb as an artifact\n\n    artifact = wandb.Artifact(f\"checkpoint_epoch_{epoch+1}\", type=\"model\")\n    artifact.add_file(path_to_checkpoint)\n    wandb.log_artifact(artifact)\n\n    print(f\"Checkpoint saved at {path_to_checkpoint}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:15:15.579447Z","iopub.execute_input":"2025-02-28T07:15:15.579779Z","iopub.status.idle":"2025-02-28T07:15:15.585512Z","shell.execute_reply.started":"2025-02-28T07:15:15.579753Z","shell.execute_reply":"2025-02-28T07:15:15.584746Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def save_entire_model(epoch, model, optimizer, checkpoint_dir):\n    \"\"\"\n    Saves the entire model (architecture + weights), optimizer, and epoch info.\n    \"\"\"\n    \n    if not os.path.exists(checkpoint_dir):\n        os.makedirs(checkpoint_dir)\n\n    checkpoint_path = os.path.join(checkpoint_dir, f'hybrid_model_epoch_{epoch}.pth')\n\n    # Save the entire model\n    torch.save({\n        'epoch': epoch,\n        'model': model,  # Saving entire model (not just state_dict)\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict()\n    }, checkpoint_path)\n\n    # Log to WandB\n    artifact = wandb.Artifact(f'hybrid_model_epoch_{epoch+1}', type='model')\n    artifact.add_file(checkpoint_path)\n    wandb.log_artifact(artifact)\n\n    print(f'Model saved at {checkpoint_path}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:15:15.763083Z","iopub.execute_input":"2025-02-28T07:15:15.763336Z","iopub.status.idle":"2025-02-28T07:15:15.769305Z","shell.execute_reply.started":"2025-02-28T07:15:15.763317Z","shell.execute_reply":"2025-02-28T07:15:15.768618Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def train_model(num_epochs, model, optimizer, criterion, train_loader, val_loader, device, checkpoint_dir=\"/kaggle/working\"):\n    for epoch in range(num_epochs):\n        train_loss, train_acc = train_one_epoch(epoch,\n                                                model,\n                                                optimizer,\n                                                criterion,\n                                                train_loader,\n                                                device)\n        val_loss, val_acc = validate_one_epoch(epoch,\n                                               model,\n                                               criterion,\n                                               val_loader,\n                                               device)\n        wandb.log({\n            \"epoch\": epoch,\n            \"training_loss\": train_loss,\n            \"training_accuracy\": train_acc,\n            \"validation_loss\": val_loss,\n            \"validation_accuracy\": val_acc,\n        })\n\n        save_checkpoint(epoch, model, optimizer, checkpoint_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:15:28.829679Z","iopub.execute_input":"2025-02-28T07:15:28.829979Z","iopub.status.idle":"2025-02-28T07:15:28.835384Z","shell.execute_reply.started":"2025-02-28T07:15:28.829958Z","shell.execute_reply":"2025-02-28T07:15:28.834593Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"# Train the model\nset ***num_epochs*** to desired number of epochs","metadata":{}},{"cell_type":"code","source":"train_model(\n    num_epochs=1, \n    model=model,\n    optimizer=optimizer,\n    criterion=criterion,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    device=device,\n    checkpoint_dir=\"/kaggle/working\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation on Test Set","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ndef evaluate_test_set(model, test_loader, device):\n    \"\"\"\n    Evaluate the model on the test set and compute accuracy.\n    \"\"\"\n\n    print('Evaluating the test set...')\n    model.eval()# Set model to evaluation mode\n    # model.train() #setting the model to train mode to see if the batchNorm are misbehaving\n\n    total = 0\n    correct = 0\n\n    with torch.no_grad():\n        progress_bar = tqdm(test_loader, desc='Testing', leave=False)\n        for images, labels in progress_bar:\n            images, labels = images.to(device), labels.to(device).float()\n\n            # Forward pass through the unified model\n            outputs = model(images, images)  # Pass the same images for both FreqNet & CLIP\n\n            # Convert logits to binary predictions\n            predicted = (torch.sigmoid(outputs) > 0.5).float()\n\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    # Compute accuracy\n    test_acc = correct / total\n    print(f'Test Accuracy: {test_acc:.4f}')\n\n    wandb.log({\"Test_Accuracy\": test_acc})\n\n    return test_acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:16:53.160448Z","iopub.execute_input":"2025-02-28T07:16:53.160766Z","iopub.status.idle":"2025-02-28T07:16:53.167281Z","shell.execute_reply.started":"2025-02-28T07:16:53.160733Z","shell.execute_reply":"2025-02-28T07:16:53.166297Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# Evaluate the trained model on the test set\ntest_accuracy = evaluate_test_set(model,\n                                  test_loader, \n                                  device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save Model\n\nset ***epoch*** to number of epochs set above","metadata":{}},{"cell_type":"code","source":"checkpoint_dir = '/kaggle/working'\nsave_entire_model(epoch = 10,\n                  model = model,\n                  optimizer = optimizer,\n                  checkpoint_dir = checkpoint_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:17:26.344050Z","iopub.execute_input":"2025-02-28T07:17:26.344373Z","iopub.status.idle":"2025-02-28T07:17:34.702444Z","shell.execute_reply.started":"2025-02-28T07:17:26.344332Z","shell.execute_reply":"2025-02-28T07:17:34.701545Z"}},"outputs":[{"name":"stdout","text":"Model saved at /kaggle/working/hybrid_model_epoch_10.pth\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# def load_entire_model(checkpoint_path, device):\n#     \"\"\"\n#     Loads the entire model from a checkpoint file.\n\n#     Parameters:\n#     - checkpoint_path (str): Path to the saved model file.\n#     - device (str): 'cuda' or 'cpu'.\n\n#     Returns:\n#     - model (nn.Module): Loaded HybridDeepfakeDetector model.\n#     - optimizer (torch.optim.Optimizer): Optimizer state.\n#     - epoch (int): Last saved epoch.\n    \n#     \"\"\"\n    \n#     checkpoint = torch.load(checkpoint_path, map_location=device)\n    \n#     # Load the saved model architecture and weights\n#     model = checkpoint['model'].to(device)\n#     model.load_state_dict(checkpoint['model_state_dict'])\n\n#     # Recreate optimizer and load its state\n#     optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n#     epoch = checkpoint['epoch']\n\n#     print(f'Model loaded from {checkpoint_path}, last trained epoch: {epoch+1}')\n    \n#     return model, optimizer, epoch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T05:33:04.109833Z","iopub.execute_input":"2025-02-22T05:33:04.110170Z","iopub.status.idle":"2025-02-22T05:33:04.115550Z","shell.execute_reply.started":"2025-02-22T05:33:04.110143Z","shell.execute_reply":"2025-02-22T05:33:04.114659Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# loaded_model, optimizer, epoch = load_entire_model(\"/kaggle/input/hybridmodel20k10ep/pytorch/default/1/hybrid_model_epoch_10.pth\", device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"def load_entire_model(checkpoint_path, device):\n    \"\"\"\n    Loads the entire model from a checkpoint file.\n\n    Parameters:\n    - checkpoint_path (str): Path to the saved model file.\n    - device (str): 'cuda' or 'cpu'.\n\n    Returns:\n    - model (nn.Module): Loaded HybridDeepfakeDetector model.\n    - optimizer (torch.optim.Optimizer): Optimizer state.\n    - epoch (int): Last saved epoch.\n    \n    \"\"\"\n    \n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    \n    # Load the saved model architecture and weights\n    model = checkpoint['model'].to(device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n\n    # Recreate optimizer and load its state\n    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n    epoch = checkpoint['epoch']\n\n    print(f'Model loaded from {checkpoint_path}, last trained epoch: {epoch+1}')\n    \n    return model, optimizer, epoch\n","metadata":{}},{"cell_type":"markdown","source":"# Visualization of embeddings","metadata":{}},{"cell_type":"code","source":"# from sklearn.decomposition import PCA\n# from sklearn.manifold import TSNE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T07:25:34.346320Z","iopub.execute_input":"2025-02-16T07:25:34.346650Z","iopub.status.idle":"2025-02-16T07:25:34.609081Z","shell.execute_reply.started":"2025-02-16T07:25:34.346627Z","shell.execute_reply":"2025-02-16T07:25:34.608181Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# def analyze_embeddings(model, train_loader, device):\n#     \"\"\"\n#     Extracts and compares embeddings from FreqNet, CLIP, and Hybrid representation.\n#     Returns embeddings and differences.\n#     \"\"\"\n\n#     model.eval()  # Set model to evaluation mode\n\n#     sampled_images = []\n#     freq_embeddings = []\n#     clip_embeddings = []\n#     hybrid_embeddings = []\n\n#     with torch.no_grad():\n#         for images, _ in train_loader:\n#             images = images.to(device)\n\n#             # Forward pass to get embeddings\n#             freq_features = model.freqnet(images)  # FreqNet embeddings\n\n#             denormalized_images = torch.stack([model.denormalize(image) for image in images])\n#             clip_features = model.extract_clip_features(denormalized_images)\n            \n#             # clip_features = model.extract_clip_features(images)  # CLIP embeddings\n\n#             # Normalize and concatenate to get hybrid features\n#             freq_features_norm = torch.nn.functional.normalize(freq_features, dim=-1)\n#             clip_features_norm = torch.nn.functional.normalize(clip_features, dim=-1)\n#             hybrid_features = torch.cat((freq_features_norm, clip_features_norm), dim=-1)\n\n#             # Store embeddings\n#             sampled_images.append(images.cpu())  # Store original images\n#             freq_embeddings.append(freq_features.cpu().numpy())  # Store FreqNet embeddings\n#             clip_embeddings.append(clip_features.cpu().numpy())  # Store CLIP embeddings\n#             hybrid_embeddings.append(hybrid_features.cpu().numpy())  # Store Hybrid embeddings\n\n#             # Stop after 6 images\n#             if len(sampled_images) >= 6:\n#                 break\n\n#     # Convert lists to numpy arrays\n#     freq_embeddings = np.vstack(freq_embeddings)[:6]\n#     clip_embeddings = np.vstack(clip_embeddings)[:6]\n#     hybrid_embeddings = np.vstack(hybrid_embeddings)[:6]\n\n#     # Compute changes in embeddings\n#     freq_hybrid_diff = np.linalg.norm(hybrid_embeddings[:, :512] - freq_embeddings, axis=1)\n#     clip_hybrid_diff = np.linalg.norm(hybrid_embeddings[:, 512:] - clip_embeddings, axis=1)\n\n#     return sampled_images, freq_embeddings, clip_embeddings, hybrid_embeddings, freq_hybrid_diff, clip_hybrid_diff\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T08:37:28.759037Z","iopub.execute_input":"2025-02-16T08:37:28.759413Z","iopub.status.idle":"2025-02-16T08:37:28.768469Z","shell.execute_reply.started":"2025-02-16T08:37:28.759386Z","shell.execute_reply":"2025-02-16T08:37:28.767478Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"# # Define a function to reverse ImageNet normalization\n# def denormalize_image(image):\n#     mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 1, 3)  # Shape (1, 1, 3)\n#     std = torch.tensor([0.229, 0.224, 0.225]).view(1, 1, 3)\n#     image = image * std + mean  # Reverse normalization\n#     return image.clamp(0, 1)  # Ensure values are in [0, 1] range\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T08:52:40.865773Z","iopub.execute_input":"2025-02-16T08:52:40.866156Z","iopub.status.idle":"2025-02-16T08:52:40.871752Z","shell.execute_reply.started":"2025-02-16T08:52:40.866124Z","shell.execute_reply":"2025-02-16T08:52:40.870886Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"# def visualize_embeddings(freq_embeddings, clip_embeddings, hybrid_embeddings, sampled_images):\n#     \"\"\"\n#     Uses PCA and t-SNE to visualize embeddings of FreqNet, CLIP, and Hybrid embeddings.\n#     \"\"\"\n\n#     # Reduce dimensionality using PCA\n#     pca = PCA(n_components=2)\n#     freq_pca = pca.fit_transform(freq_embeddings)\n#     clip_pca = pca.fit_transform(clip_embeddings)\n#     hybrid_pca = pca.fit_transform(hybrid_embeddings)\n\n#     # Further reduce with t-SNE\n#     tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n#     freq_tsne = tsne.fit_transform(freq_embeddings)\n#     clip_tsne = tsne.fit_transform(clip_embeddings)\n#     hybrid_tsne = tsne.fit_transform(hybrid_embeddings)\n\n#     fig, axs = plt.subplots(2, 3, figsize=(15, 10))  # Ensure correct layout\n\n#     #Flatten axs for safe indexing\n#     axs = axs.flatten()\n\n#     #Extract individual images before permute\n#     for i in range(len(sampled_images)):  # Ensure we do not exceed available indices\n#         image = sampled_images[i][0]  # Extract first image from batch\n#         image = image.permute(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n\n#         image = denormalize_image(image)\n        \n#         # Convert embeddings to rounded text for display\n#         clip_text = f\"CLIP: {np.round(clip_embeddings[i][:3], 3)}\"  # Show first 5 values\n#         freq_text = f\"FreqNet: {np.round(freq_embeddings[i][:3], 3)}\"\n#         hybrid_text = f\"Hybrid: {np.round(hybrid_embeddings[i][:3], 3)}\"\n\n#         axs[i].imshow(image.cpu().numpy())  # Convert from tensor\n#         axs[i].axis(\"off\")\n#         axs[i].set_title(f\"{clip_text}\\n{freq_text}\\n{hybrid_text}\", fontsize=8)\n\n#     # Plot PCA visualization\n#     plt.figure(figsize=(8, 6))\n#     plt.scatter(freq_pca[:, 0], freq_pca[:, 1], label=\"FreqNet\", color=\"blue\")\n#     plt.scatter(clip_pca[:, 0], clip_pca[:, 1], label=\"CLIP\", color=\"red\")\n#     plt.scatter(hybrid_pca[:, 0], hybrid_pca[:, 1], label=\"Hybrid\", color=\"green\")\n#     plt.title(\"PCA Projection\")\n#     plt.legend()\n#     plt.show()\n\n#     # Plot t-SNE visualization\n#     plt.figure(figsize=(8, 6))\n#     plt.scatter(freq_tsne[:, 0], freq_tsne[:, 1], label=\"FreqNet\", color=\"blue\")\n#     plt.scatter(clip_tsne[:, 0], clip_tsne[:, 1], label=\"CLIP\", color=\"red\")\n#     plt.scatter(hybrid_tsne[:, 0], hybrid_tsne[:, 1], label=\"Hybrid\", color=\"green\")\n#     plt.title(\"t-SNE Projection\")\n#     plt.legend()\n#     plt.show()\n    \n#     # Plot embedding changes\n#     plt.figure(figsize=(8, 6))\n#     plt.bar(range(len(sampled_images)), np.linalg.norm(hybrid_embeddings[:, :512] - freq_embeddings, axis=1), color=\"blue\", label=\"FreqNet → Hybrid\")\n#     plt.bar(range(len(sampled_images)), np.linalg.norm(hybrid_embeddings[:, 512:] - clip_embeddings, axis=1), color=\"red\", label=\"CLIP → Hybrid\")\n#     plt.title(\"Embedding Change Magnitude\")\n#     plt.legend()\n#     plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T08:52:41.124092Z","iopub.execute_input":"2025-02-16T08:52:41.124439Z","iopub.status.idle":"2025-02-16T08:52:41.135638Z","shell.execute_reply.started":"2025-02-16T08:52:41.124415Z","shell.execute_reply":"2025-02-16T08:52:41.134704Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"# # Extract embeddings\n# sampled_images, freq_embeddings, clip_embeddings, hybrid_embeddings, freq_hybrid_diff, clip_hybrid_diff = analyze_embeddings(model, train_loader, device)\n\n# # Visualize embeddings\n# visualize_embeddings(freq_embeddings, clip_embeddings, hybrid_embeddings, sampled_images)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"def load_checkpoint(checkpoint_path, model, optimizer, device):\n    \"\"\"\n    \n    Loads a checkpoint to resume training.\n    Parameters:\n    - checkpoint_path (str): Path to the saved checkpoint file.\n    - model (HybridDeepfakeDetector): Model to load weights into.\n    - optimizer (torch.optim.Optimizer): Optimizer to load state into.\n    - device (str): 'cuda' or 'cpu'.\n    \n    Returns:\n    - int: Last epoch number to resume training.\n    \n    \"\"\"\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n    last_epoch = checkpoint[\"epoch\"]\n\n    print(f\"Resuming training from epoch {last_epoch}\")\n    return last_epoch\n\n\"\"\"\n\ncheckpoint_path = \"./checkpoints/checkpoint_epoch_5.pth\"  # Change to the latest checkpoint\nlast_epoch = load_checkpoint(checkpoint_path, model, optimizer, device)\ntrain_model(num_epochs=10 - last_epoch, model=model, optimizer=optimizer, criterion=criterion, train_loader=train_loader, val_loader=val_loader, device=device, checkpoint_dir=\"./checkpoints\")\n\n\"\"\"\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}