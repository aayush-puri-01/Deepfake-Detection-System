{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10597785,"sourceType":"datasetVersion","datasetId":6559539},{"sourceId":243753,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":208219,"modelId":229916},{"sourceId":243769,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":208234,"modelId":229930},{"sourceId":262709,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":224664,"modelId":246412},{"sourceId":270758,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":208234,"modelId":229930}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import random_split\nfrom torch.autograd import Variable\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import precision_recall_fscore_support, confusion_matrix\nimport numpy as np\nimport torchvision\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:13:22.474441Z","iopub.execute_input":"2025-03-01T15:13:22.474819Z","iopub.status.idle":"2025-03-01T15:13:41.604388Z","shell.execute_reply.started":"2025-03-01T15:13:22.474790Z","shell.execute_reply":"2025-03-01T15:13:41.603606Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from transformers import CLIPProcessor, CLIPModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:13:41.605479Z","iopub.execute_input":"2025-03-01T15:13:41.606118Z","iopub.status.idle":"2025-03-01T15:13:43.664787Z","shell.execute_reply.started":"2025-03-01T15:13:41.606087Z","shell.execute_reply":"2025-03-01T15:13:43.663945Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:13:43.666338Z","iopub.execute_input":"2025-03-01T15:13:43.666852Z","iopub.status.idle":"2025-03-01T15:13:43.732231Z","shell.execute_reply.started":"2025-03-01T15:13:43.666827Z","shell.execute_reply":"2025-03-01T15:13:43.731106Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import kagglehub\npath_to_clipped_freqnet = kagglehub.model_download('aayushpuri01/clipped-freqnet/PyTorch/default/1')\nprint(path_to_clipped_freqnet)\nos.chdir(path_to_clipped_freqnet)\nfrom clipped_freqnet import freqnet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:13:43.734036Z","iopub.execute_input":"2025-03-01T15:13:43.734444Z","iopub.status.idle":"2025-03-01T15:13:44.536566Z","shell.execute_reply.started":"2025-03-01T15:13:43.734406Z","shell.execute_reply":"2025-03-01T15:13:44.535801Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/clipped-freqnet/pytorch/default/1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"path_to_dataloader = kagglehub.model_download('aayushpuri01/dataloader-pipeline-for-dds/PyTorch/default/2')\nprint(path_to_dataloader)\nos.chdir(path_to_dataloader)\nfrom dataloading_pipeline_fixleakage import DataPipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:13:44.537394Z","iopub.execute_input":"2025-03-01T15:13:44.537672Z","iopub.status.idle":"2025-03-01T15:13:45.031790Z","shell.execute_reply.started":"2025-03-01T15:13:44.537642Z","shell.execute_reply":"2025-03-01T15:13:45.030904Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/dataloader-pipeline-for-dds/pytorch/default/2\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"os.chdir('/kaggle/working')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:13:45.032739Z","iopub.execute_input":"2025-03-01T15:13:45.033014Z","iopub.status.idle":"2025-03-01T15:13:45.036365Z","shell.execute_reply.started":"2025-03-01T15:13:45.032993Z","shell.execute_reply":"2025-03-01T15:13:45.035564Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Wandb config","metadata":{}},{"cell_type":"code","source":"import wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=api_key)\n    anony = None\nexcept:\n    anony = \"must\"\n    \n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:13:45.037337Z","iopub.execute_input":"2025-03-01T15:13:45.037634Z","iopub.status.idle":"2025-03-01T15:13:54.213505Z","shell.execute_reply.started":"2025-03-01T15:13:45.037606Z","shell.execute_reply":"2025-03-01T15:13:54.212533Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maayushpuri2486\u001b[0m (\u001b[33maayushpuri2486-pulchowk-campus\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"wandb.init(\n    # set the wandb project where this run will be logged\n    project=\"embedding scaling first test\",\n\n    # track hyperparameters and run metadata\n    config={\n    \"learning_rate\": 0.001,\n    \"architecture\": \"Freqnet+CLIP\",\n    \"dataset\": \"Custom Augmented Dataset\",\n    \"epochs\": 10,\n    }\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:14:10.623446Z","iopub.execute_input":"2025-03-01T15:14:10.624125Z","iopub.status.idle":"2025-03-01T15:14:18.130840Z","shell.execute_reply.started":"2025-03-01T15:14:10.624096Z","shell.execute_reply":"2025-03-01T15:14:18.129999Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250301_151410-rkvsogsq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/aayushpuri2486-pulchowk-campus/embedding%20scaling%20first%20test/runs/rkvsogsq' target=\"_blank\">efficient-sun-1</a></strong> to <a href='https://wandb.ai/aayushpuri2486-pulchowk-campus/embedding%20scaling%20first%20test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/aayushpuri2486-pulchowk-campus/embedding%20scaling%20first%20test' target=\"_blank\">https://wandb.ai/aayushpuri2486-pulchowk-campus/embedding%20scaling%20first%20test</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/aayushpuri2486-pulchowk-campus/embedding%20scaling%20first%20test/runs/rkvsogsq' target=\"_blank\">https://wandb.ai/aayushpuri2486-pulchowk-campus/embedding%20scaling%20first%20test/runs/rkvsogsq</a>"},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aayushpuri2486-pulchowk-campus/embedding%20scaling%20first%20test/runs/rkvsogsq?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7cb074afc6a0>"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"# Dataloader Configuration","metadata":{}},{"cell_type":"markdown","source":"The DataPipeline function takes the following arguments:\n\n1. ***path_to_drive*** and ***data_dir*** are joined to make the complete relative directory path,\n   so just split if you have a complete path\n2. ***num_images*** (default value set to 20,000): create a subset of the dataset, i.e. 20,000 images will\n   be used for training and validation set\n3. ***val_split*** set for the portion of limited dataset to use for validation\n4. ***batch_size***\n5. ***test_size*** (default set to 3000): creates a separate test set of 3000 images. \n\nValues can be changed for experimentations. \n","metadata":{}},{"cell_type":"code","source":"pipeline = DataPipeline(path_to_drive='/kaggle/input',\n                        data_dir='deepfake-dataset/Deepfake_Dataset',\n                        val_split=0.3,\n                        batch_size=32,\n                        num_images=12000,\n                        test_size =2000)\ntrain_loader, val_loader, test_loader = pipeline.get_loaders()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:14:18.131900Z","iopub.execute_input":"2025-03-01T15:14:18.132121Z","iopub.status.idle":"2025-03-01T15:15:00.840823Z","shell.execute_reply.started":"2025-03-01T15:14:18.132102Z","shell.execute_reply":"2025-03-01T15:15:00.839888Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"#Visualising some images from the dataloaders, \n#try all the data loaders \n#comment the cell when running the session \n\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ndef denormalized(img_tensor, mean, std):\n    img_tensor = img_tensor.clone()  # cloning the tensor to avoid changing it\n    for t, m, s in zip(img_tensor, mean, std):\n        t.mul_(s).add_(m)  # Denormalize each channel\n    return img_tensor\n\ndata_iter = iter(val_loader)\nimages, labels = next(data_iter)\nimages = torch.stack([denormalized(img, mean, std) for img in images])\nimages = images.numpy().transpose((0, 2, 3, 1))\n\nfig, axes = plt.subplots(3, 3, figsize = (8, 6))\nfor i in range(3):\n  for j in range(3):\n    axes[i][j].imshow(images[i*3+j])\n    lbl = labels[i*3+j].item()\n    if lbl == 0:\n      axes[i][j].set_title(\"Fake\")\n    else:\n      axes[i][j].set_title(\"Real\")\n    axes[i][j].axis('off')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pipeline = DataPipeline(path_to_drive='/kaggle/input',\n#                         data_dir='deepfake-dataset/Deepfake_Dataset',\n#                         val_split=0.3,\n#                         batch_size=32,\n#                         num_images=20000)\n# train_loader, val_loader, test_loader = pipeline.get_loaders()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Architecture Config and Init","metadata":{}},{"cell_type":"code","source":"class HybridDeepfakeDetector(nn.Module):\n    def __init__(self, freq_model, clip_model_name=\"openai/clip-vit-large-patch14\", device=\"cuda\", use_scaling=True):\n        super(HybridDeepfakeDetector, self).__init__()\n        self.device = device\n        self.use_scaling = use_scaling\n\n        if self.use_scaling:\n            self.alpha = nn.Parameter(torch.tensor(0.5))\n            self.beta = nn.Parameter(torch.tensor(0.5))\n\n        self.norm = nn.BatchNorm1d(512 + 768)\n        self.classifier_hidden_layer = nn.Linear(512+768, 512)\n        self.activation = nn.ReLU()\n        self.classifier_output_layer = nn.Linear(512, 1)\n\n        # Load FreqNet\n        self.freqnet = freq_model.to(self.device)\n\n        # Load CLIP Model\n        self.clip_model = CLIPModel.from_pretrained(clip_model_name).to(self.device)\n        self.clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n\n        # Define Fully Connected Classifier\n        self.fc = nn.Linear(512 + 768, 1)  # FreqNet (512) + CLIP (768)\n\n        # ImageNet Denormalization\n        imagenet_mean = [0.485, 0.456, 0.406]\n        imagenet_std = [0.229, 0.224, 0.225]\n        self.denormalize = transforms.Compose([\n            transforms.Normalize(mean=[-m / s for m, s in zip(imagenet_mean, imagenet_std)],\n                                 std=[1 / s for s in imagenet_std]),\n            # transforms.Lambda(lambda x: x.clamp(0, 1))  # Clamp to [0,1]\n            transforms.Lambda(HybridDeepfakeDetector.clamp_image)\n        ])\n\n    @staticmethod\n    def clamp_image(tensor):\n        return tensor.clamp(0, 1)\n    \n    def extract_clip_features(self, images):\n        \"\"\"\n        Extract CLIP embeddings from images.\n        \"\"\"\n        inputs = self.clip_processor(images=images, return_tensors=\"pt\", padding=True)\n        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n        with torch.no_grad():\n            embeddings = self.clip_model.get_image_features(**inputs)\n        return embeddings.to(self.device)\n\n    def forward(self, freq_input, clip_input):\n        \"\"\"\n        freq_input: Image tensors normalized with ImageNet stats for FreqNet\n        clip_input: Same image tensors (but denormalized) for CLIP\n        \"\"\"\n        # Get frequency-based features from FreqNet\n        freq_features = self.freqnet(freq_input)\n\n        # Get semantic embeddings from CLIP\n        denormalized_images = torch.stack([self.denormalize(image) for image in clip_input])\n        clip_features = self.extract_clip_features(denormalized_images)\n\n        # Normalize features and concatenate\n        freq_features = F.normalize(freq_features, dim=-1)\n        clip_features = F.normalize(clip_features, dim=-1)\n\n        if self.use_scaling: \n            freq_features = self.alpha * freq_features\n            clip_features = self.beta * clip_features\n        \n        combined_features = torch.cat((freq_features, clip_features), dim=-1)\n\n        combined_features = self.norm(combined_features)\n\n        logits = self.classifier_hidden_layer(combined_features)\n\n        # Classifier output\n        logits = self.classifier_output_layer(logits)\n        return logits.squeeze()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:15:00.842265Z","iopub.execute_input":"2025-03-01T15:15:00.842496Z","iopub.status.idle":"2025-03-01T15:15:00.853062Z","shell.execute_reply.started":"2025-03-01T15:15:00.842469Z","shell.execute_reply":"2025-03-01T15:15:00.852289Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"#Initialize the models\nfreqnet_model = freqnet(num_classes=2)  # Assuming `freqnet` function initializes FreqNet\nmodel = HybridDeepfakeDetector(freqnet_model, device=device).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:15:00.853855Z","iopub.execute_input":"2025-03-01T15:15:00.854055Z","iopub.status.idle":"2025-03-01T15:15:17.796606Z","shell.execute_reply.started":"2025-03-01T15:15:00.854037Z","shell.execute_reply":"2025-03-01T15:15:17.795716Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.52k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9410642778854d4fa39090a626723917"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbe355ab2dbd408492b047b5e72e1a80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34ab9f3ed9dd4581adf1cd4bf636ac49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bc0eacecf0542f191bf101e7c6435ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcb115e29cc24d86916cef3598484c89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48f03a4d13254610b0a205b0d1aa3af2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a84d08a3f7348df89f2f3944ea1bafb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a347beb62ec47e58347de3b4da4e757"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Optimizer & Loss\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.001) #set lr = 0.001 optimally? \ncriterion = nn.BCEWithLogitsLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:15:17.797388Z","iopub.execute_input":"2025-03-01T15:15:17.797624Z","iopub.status.idle":"2025-03-01T15:15:17.805438Z","shell.execute_reply.started":"2025-03-01T15:15:17.797604Z","shell.execute_reply":"2025-03-01T15:15:17.804595Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Helper Functions","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(epoch, model, optimizer, criterion, train_loader, device):\n    model.train()\n    running_loss = 0.0\n    running_corrects = 0\n\n    loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=True)\n    \n    for batch_idx, (images, labels) in loop:\n        images, labels = images.to(device), labels.to(device).float()\n\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(images, images)  # Passing same images to both FreqNet & CLIP\n        loss = criterion(outputs, labels)\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n\n        # Track loss and accuracy\n        running_loss += loss.item() * images.size(0)\n        predicted = (torch.sigmoid(outputs) > 0.5).float()\n        running_corrects += (predicted == labels).sum().item()\n\n        loop.set_description(f'Epoch [{epoch+1}]')\n        loop.set_postfix(loss=loss.item())\n\n    epoch_loss = running_loss / len(train_loader.dataset)\n    epoch_acc = running_corrects / len(train_loader.dataset)\n    print(f'Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.4f}')\n    return epoch_loss, epoch_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:16:03.378446Z","iopub.execute_input":"2025-03-01T15:16:03.378817Z","iopub.status.idle":"2025-03-01T15:16:03.386161Z","shell.execute_reply.started":"2025-03-01T15:16:03.378787Z","shell.execute_reply":"2025-03-01T15:16:03.385299Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def validate_one_epoch(epoch, model, criterion, val_loader, device):\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        progress_bar = tqdm(val_loader, desc=\"Validating\", leave=False)\n        for images, labels in progress_bar:\n            images, labels = images.to(device), labels.to(device).float()\n            outputs = model(images, images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * images.size(0)\n\n            # Predictions\n            predicted = (torch.sigmoid(outputs) > 0.5).float()\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            progress_bar.set_postfix({'Val Loss': loss.item()})\n\n    val_acc = correct / total\n    print(f'Validation Loss: {val_loss / len(val_loader.dataset):.4f}, Validation Accuracy: {val_acc:.4f}')\n    return val_loss / len(val_loader.dataset), val_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:16:05.836155Z","iopub.execute_input":"2025-03-01T15:16:05.836472Z","iopub.status.idle":"2025-03-01T15:16:05.843284Z","shell.execute_reply.started":"2025-03-01T15:16:05.836447Z","shell.execute_reply":"2025-03-01T15:16:05.842434Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def save_checkpoint(epoch, model, optimizer, checkpoint_dir = '/kaggle/working'):\n    os.makedirs(checkpoint_dir, exist_ok = True)\n\n    path_to_checkpoint = os.path.join(checkpoint_dir, f\"scaled_epoch_{epoch+1}.pth\")\n\n    #saving the checkpoint dictionary with the model and optimizer state dict only for resuming training if deemed necesary\n\n    torch.save({\n        \"epoch\": epoch + 1, \n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n    }, path_to_checkpoint)\n\n    #might as well log to wandb as an artifact\n\n    # artifact = wandb.Artifact(f\"checkpoint_epoch_{epoch+1}\", type=\"model\")\n    # artifact.add_file(path_to_checkpoint)\n    # wandb.log_artifact(artifact)\n\n    print(f\"Checkpoint saved at {path_to_checkpoint}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:19:08.121563Z","iopub.execute_input":"2025-03-01T15:19:08.121958Z","iopub.status.idle":"2025-03-01T15:19:08.128056Z","shell.execute_reply.started":"2025-03-01T15:19:08.121928Z","shell.execute_reply":"2025-03-01T15:19:08.127025Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def save_entire_model(epoch, model, optimizer, checkpoint_dir):\n    \"\"\"\n    Saves the entire model (architecture + weights), optimizer, and epoch info.\n    \"\"\"\n    \n    if not os.path.exists(checkpoint_dir):\n        os.makedirs(checkpoint_dir)\n\n    checkpoint_path = os.path.join(checkpoint_dir, f'hybrid_model_epoch_{epoch}.pth')\n\n    # Save the entire model\n    torch.save({\n        'epoch': epoch,\n        'model': model,  # Saving entire model (not just state_dict)\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict()\n    }, checkpoint_path)\n\n    # Log to WandB\n    # artifact = wandb.Artifact(f'hybrid_model_epoch_{epoch+1}', type='model')\n    # artifact.add_file(checkpoint_path)\n    # wandb.log_artifact(artifact)\n\n    print(f'Model saved at {checkpoint_path}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:19:20.000471Z","iopub.execute_input":"2025-03-01T15:19:20.000847Z","iopub.status.idle":"2025-03-01T15:19:20.006687Z","shell.execute_reply.started":"2025-03-01T15:19:20.000817Z","shell.execute_reply":"2025-03-01T15:19:20.005692Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def train_model(num_epochs, model, optimizer, criterion, train_loader, val_loader, device, checkpoint_dir=\"/kaggle/working\"):\n    for epoch in range(num_epochs):\n        train_loss, train_acc = train_one_epoch(epoch,\n                                                model,\n                                                optimizer,\n                                                criterion,\n                                                train_loader,\n                                                device)\n        val_loss, val_acc = validate_one_epoch(epoch,\n                                               model,\n                                               criterion,\n                                               val_loader,\n                                               device)\n        wandb.log({\n            \"epoch\": epoch,\n            \"training_loss\": train_loss,\n            \"training_accuracy\": train_acc,\n            \"validation_loss\": val_loss,\n            \"validation_accuracy\": val_acc,\n        })\n\n        save_checkpoint(epoch, model, optimizer, checkpoint_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:19:25.506193Z","iopub.execute_input":"2025-03-01T15:19:25.506499Z","iopub.status.idle":"2025-03-01T15:19:25.512435Z","shell.execute_reply.started":"2025-03-01T15:19:25.506476Z","shell.execute_reply":"2025-03-01T15:19:25.511597Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# Train the model\nset ***num_epochs*** to desired number of epochs","metadata":{}},{"cell_type":"code","source":"train_model(\n    num_epochs=10, \n    model=model,\n    optimizer=optimizer,\n    criterion=criterion,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    device=device,\n    checkpoint_dir=\"/kaggle/working\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:19:33.497292Z","iopub.execute_input":"2025-03-01T15:19:33.497579Z","execution_failed":"2025-03-01T16:08:38.570Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/263 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\nEpoch [1]: 100%|██████████| 263/263 [08:38<00:00,  1.97s/it, loss=0.22]  \n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4013, Training Accuracy: 0.8208\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.8062, Validation Accuracy: 0.8325\nCheckpoint saved at /kaggle/working/scaled_epoch_1.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch [2]: 100%|██████████| 263/263 [08:32<00:00,  1.95s/it, loss=0.426] \n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2042, Training Accuracy: 0.9127\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 1.2881, Validation Accuracy: 0.6536\nCheckpoint saved at /kaggle/working/scaled_epoch_2.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch [3]: 100%|██████████| 263/263 [08:30<00:00,  1.94s/it, loss=0.128] \n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1584, Training Accuracy: 0.9388\n","output_type":"stream"},{"name":"stderr","text":"                                                                              \r","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.1021, Validation Accuracy: 0.9658\nCheckpoint saved at /kaggle/working/scaled_epoch_3.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch [4]: 100%|██████████| 263/263 [08:30<00:00,  1.94s/it, loss=0.0572] \n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1238, Training Accuracy: 0.9520\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.5600, Validation Accuracy: 0.7833\nCheckpoint saved at /kaggle/working/scaled_epoch_4.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch [5]:  47%|████▋     | 123/263 [03:58<04:30,  1.93s/it, loss=0.137] ","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation on Test Set","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ndef evaluate_test_set(model, test_loader, device):\n    \"\"\"\n    Evaluate the model on the test set and compute accuracy.\n    \"\"\"\n\n    print('Evaluating the test set...')\n    model.eval()# Set model to evaluation mode\n    # model.train() #setting the model to train mode to see if the batchNorm are misbehaving\n\n    total = 0\n    correct = 0\n\n    with torch.no_grad():\n        progress_bar = tqdm(test_loader, desc='Testing', leave=False)\n        for images, labels in progress_bar:\n            images, labels = images.to(device), labels.to(device).float()\n\n            # Forward pass through the unified model\n            outputs = model(images, images)  # Pass the same images for both FreqNet & CLIP\n\n            # Convert logits to binary predictions\n            predicted = (torch.sigmoid(outputs) > 0.5).float()\n\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    # Compute accuracy\n    test_acc = correct / total\n    print(f'Test Accuracy: {test_acc:.4f}')\n\n    wandb.log({\"Test_Accuracy\": test_acc})\n\n    return test_acc\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the trained model on the test set\ntest_accuracy = evaluate_test_set(model,\n                                  test_loader, \n                                  device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save Model\n\nset ***epoch*** to number of epochs set above","metadata":{}},{"cell_type":"code","source":"checkpoint_dir = '/kaggle/working'\nsave_entire_model(epoch = 10,\n                  model = model,\n                  optimizer = optimizer,\n                  checkpoint_dir = checkpoint_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def load_entire_model(checkpoint_path, device):\n#     \"\"\"\n#     Loads the entire model from a checkpoint file.\n\n#     Parameters:\n#     - checkpoint_path (str): Path to the saved model file.\n#     - device (str): 'cuda' or 'cpu'.\n\n#     Returns:\n#     - model (nn.Module): Loaded HybridDeepfakeDetector model.\n#     - optimizer (torch.optim.Optimizer): Optimizer state.\n#     - epoch (int): Last saved epoch.\n    \n#     \"\"\"\n    \n#     checkpoint = torch.load(checkpoint_path, map_location=device)\n    \n#     # Load the saved model architecture and weights\n#     model = checkpoint['model'].to(device)\n#     model.load_state_dict(checkpoint['model_state_dict'])\n\n#     # Recreate optimizer and load its state\n#     optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n#     epoch = checkpoint['epoch']\n\n#     print(f'Model loaded from {checkpoint_path}, last trained epoch: {epoch+1}')\n    \n#     return model, optimizer, epoch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# loaded_model, optimizer, epoch = load_entire_model(\"/kaggle/input/hybridmodel20k10ep/pytorch/default/1/hybrid_model_epoch_10.pth\", device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"def load_entire_model(checkpoint_path, device):\n    \"\"\"\n    Loads the entire model from a checkpoint file.\n\n    Parameters:\n    - checkpoint_path (str): Path to the saved model file.\n    - device (str): 'cuda' or 'cpu'.\n\n    Returns:\n    - model (nn.Module): Loaded HybridDeepfakeDetector model.\n    - optimizer (torch.optim.Optimizer): Optimizer state.\n    - epoch (int): Last saved epoch.\n    \n    \"\"\"\n    \n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    \n    # Load the saved model architecture and weights\n    model = checkpoint['model'].to(device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n\n    # Recreate optimizer and load its state\n    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n    epoch = checkpoint['epoch']\n\n    print(f'Model loaded from {checkpoint_path}, last trained epoch: {epoch+1}')\n    \n    return model, optimizer, epoch\n","metadata":{}},{"cell_type":"markdown","source":"# Visualization of embeddings","metadata":{}},{"cell_type":"code","source":"# from sklearn.decomposition import PCA\n# from sklearn.manifold import TSNE","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def analyze_embeddings(model, train_loader, device):\n#     \"\"\"\n#     Extracts and compares embeddings from FreqNet, CLIP, and Hybrid representation.\n#     Returns embeddings and differences.\n#     \"\"\"\n\n#     model.eval()  # Set model to evaluation mode\n\n#     sampled_images = []\n#     freq_embeddings = []\n#     clip_embeddings = []\n#     hybrid_embeddings = []\n\n#     with torch.no_grad():\n#         for images, _ in train_loader:\n#             images = images.to(device)\n\n#             # Forward pass to get embeddings\n#             freq_features = model.freqnet(images)  # FreqNet embeddings\n\n#             denormalized_images = torch.stack([model.denormalize(image) for image in images])\n#             clip_features = model.extract_clip_features(denormalized_images)\n            \n#             # clip_features = model.extract_clip_features(images)  # CLIP embeddings\n\n#             # Normalize and concatenate to get hybrid features\n#             freq_features_norm = torch.nn.functional.normalize(freq_features, dim=-1)\n#             clip_features_norm = torch.nn.functional.normalize(clip_features, dim=-1)\n#             hybrid_features = torch.cat((freq_features_norm, clip_features_norm), dim=-1)\n\n#             # Store embeddings\n#             sampled_images.append(images.cpu())  # Store original images\n#             freq_embeddings.append(freq_features.cpu().numpy())  # Store FreqNet embeddings\n#             clip_embeddings.append(clip_features.cpu().numpy())  # Store CLIP embeddings\n#             hybrid_embeddings.append(hybrid_features.cpu().numpy())  # Store Hybrid embeddings\n\n#             # Stop after 6 images\n#             if len(sampled_images) >= 6:\n#                 break\n\n#     # Convert lists to numpy arrays\n#     freq_embeddings = np.vstack(freq_embeddings)[:6]\n#     clip_embeddings = np.vstack(clip_embeddings)[:6]\n#     hybrid_embeddings = np.vstack(hybrid_embeddings)[:6]\n\n#     # Compute changes in embeddings\n#     freq_hybrid_diff = np.linalg.norm(hybrid_embeddings[:, :512] - freq_embeddings, axis=1)\n#     clip_hybrid_diff = np.linalg.norm(hybrid_embeddings[:, 512:] - clip_embeddings, axis=1)\n\n#     return sampled_images, freq_embeddings, clip_embeddings, hybrid_embeddings, freq_hybrid_diff, clip_hybrid_diff\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Define a function to reverse ImageNet normalization\n# def denormalize_image(image):\n#     mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 1, 3)  # Shape (1, 1, 3)\n#     std = torch.tensor([0.229, 0.224, 0.225]).view(1, 1, 3)\n#     image = image * std + mean  # Reverse normalization\n#     return image.clamp(0, 1)  # Ensure values are in [0, 1] range\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def visualize_embeddings(freq_embeddings, clip_embeddings, hybrid_embeddings, sampled_images):\n#     \"\"\"\n#     Uses PCA and t-SNE to visualize embeddings of FreqNet, CLIP, and Hybrid embeddings.\n#     \"\"\"\n\n#     # Reduce dimensionality using PCA\n#     pca = PCA(n_components=2)\n#     freq_pca = pca.fit_transform(freq_embeddings)\n#     clip_pca = pca.fit_transform(clip_embeddings)\n#     hybrid_pca = pca.fit_transform(hybrid_embeddings)\n\n#     # Further reduce with t-SNE\n#     tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n#     freq_tsne = tsne.fit_transform(freq_embeddings)\n#     clip_tsne = tsne.fit_transform(clip_embeddings)\n#     hybrid_tsne = tsne.fit_transform(hybrid_embeddings)\n\n#     fig, axs = plt.subplots(2, 3, figsize=(15, 10))  # Ensure correct layout\n\n#     #Flatten axs for safe indexing\n#     axs = axs.flatten()\n\n#     #Extract individual images before permute\n#     for i in range(len(sampled_images)):  # Ensure we do not exceed available indices\n#         image = sampled_images[i][0]  # Extract first image from batch\n#         image = image.permute(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n\n#         image = denormalize_image(image)\n        \n#         # Convert embeddings to rounded text for display\n#         clip_text = f\"CLIP: {np.round(clip_embeddings[i][:3], 3)}\"  # Show first 5 values\n#         freq_text = f\"FreqNet: {np.round(freq_embeddings[i][:3], 3)}\"\n#         hybrid_text = f\"Hybrid: {np.round(hybrid_embeddings[i][:3], 3)}\"\n\n#         axs[i].imshow(image.cpu().numpy())  # Convert from tensor\n#         axs[i].axis(\"off\")\n#         axs[i].set_title(f\"{clip_text}\\n{freq_text}\\n{hybrid_text}\", fontsize=8)\n\n#     # Plot PCA visualization\n#     plt.figure(figsize=(8, 6))\n#     plt.scatter(freq_pca[:, 0], freq_pca[:, 1], label=\"FreqNet\", color=\"blue\")\n#     plt.scatter(clip_pca[:, 0], clip_pca[:, 1], label=\"CLIP\", color=\"red\")\n#     plt.scatter(hybrid_pca[:, 0], hybrid_pca[:, 1], label=\"Hybrid\", color=\"green\")\n#     plt.title(\"PCA Projection\")\n#     plt.legend()\n#     plt.show()\n\n#     # Plot t-SNE visualization\n#     plt.figure(figsize=(8, 6))\n#     plt.scatter(freq_tsne[:, 0], freq_tsne[:, 1], label=\"FreqNet\", color=\"blue\")\n#     plt.scatter(clip_tsne[:, 0], clip_tsne[:, 1], label=\"CLIP\", color=\"red\")\n#     plt.scatter(hybrid_tsne[:, 0], hybrid_tsne[:, 1], label=\"Hybrid\", color=\"green\")\n#     plt.title(\"t-SNE Projection\")\n#     plt.legend()\n#     plt.show()\n    \n#     # Plot embedding changes\n#     plt.figure(figsize=(8, 6))\n#     plt.bar(range(len(sampled_images)), np.linalg.norm(hybrid_embeddings[:, :512] - freq_embeddings, axis=1), color=\"blue\", label=\"FreqNet → Hybrid\")\n#     plt.bar(range(len(sampled_images)), np.linalg.norm(hybrid_embeddings[:, 512:] - clip_embeddings, axis=1), color=\"red\", label=\"CLIP → Hybrid\")\n#     plt.title(\"Embedding Change Magnitude\")\n#     plt.legend()\n#     plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Extract embeddings\n# sampled_images, freq_embeddings, clip_embeddings, hybrid_embeddings, freq_hybrid_diff, clip_hybrid_diff = analyze_embeddings(model, train_loader, device)\n\n# # Visualize embeddings\n# visualize_embeddings(freq_embeddings, clip_embeddings, hybrid_embeddings, sampled_images)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"def load_checkpoint(checkpoint_path, model, optimizer, device):\n    \"\"\"\n    \n    Loads a checkpoint to resume training.\n    Parameters:\n    - checkpoint_path (str): Path to the saved checkpoint file.\n    - model (HybridDeepfakeDetector): Model to load weights into.\n    - optimizer (torch.optim.Optimizer): Optimizer to load state into.\n    - device (str): 'cuda' or 'cpu'.\n    \n    Returns:\n    - int: Last epoch number to resume training.\n    \n    \"\"\"\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n    last_epoch = checkpoint[\"epoch\"]\n\n    print(f\"Resuming training from epoch {last_epoch}\")\n    return last_epoch\n\n\"\"\"\n\ncheckpoint_path = \"./checkpoints/checkpoint_epoch_5.pth\"  # Change to the latest checkpoint\nlast_epoch = load_checkpoint(checkpoint_path, model, optimizer, device)\ntrain_model(num_epochs=10 - last_epoch, model=model, optimizer=optimizer, criterion=criterion, train_loader=train_loader, val_loader=val_loader, device=device, checkpoint_dir=\"./checkpoints\")\n\n\"\"\"\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}